Hoeffding's Inequality

背景：我们要统计一个学校中男生和女生的比例，由于人数众多我们无法全部统计，而我们可以统计在一段时间内走出校门的男生和女生的比例，这个比例很有可能接近整个学校总体的男女分布。

假设有一个已知的模型h，它在总体数据上的错误率为$\mu$，现在我们有一个从总体中采样出的N个样本，利用这个模型h，去预测这N个样本的标签和实际标签的错误率，表示为$\nu$。根据大数定律，$\nu$可以近似估计$\mu$，当N很大时，$\mu$和$\nu$满足Hoeffding不等式
$$
\mathbb{P}[|\nu-\mu|>\epsilon] \leq 2 \exp \left(-2 \epsilon^{2} N\right)
$$
即$\nu=\mu$是PAC的（probably approximately correct）

$\epsilon$是用户的容忍度。

它想表达的意思是，当我们想考察h在样本上的错误率$\nu$和总体上的错误率$\mu$之间的差别时，我们设置一个范围$\epsilon$，它们的差距要大于$\epsilon$的概率（逃逸概率）存在上界，由样本数N和容忍度$\epsilon$所决定。当样本数越大，上界越低。容忍度越大，上界越低（但是这是自欺欺人的表现）。这个上界不需要知道总体的参数$\mu$，因此不需要对总体有所假设。

所以根据Hoeffding不等式，可以保证上文中利用一段时间内经过校门口的男女生比例，来代表全校的男女比例的这种方法是PAC的。

需要注意的是，Hoeffding保证的是，对于一个模型h，可以保证它的错误率$\nu$与f的错误率$\mu$接近。但是当我**可以对模型h进行选择**时，这个选出的h在样本N上的错误率$\nu$则无法保证它在总体数据集上的错误率还很低。

具体来说，对于某一个模型h，样本内的错误率$E_{in}(h)$和样本外的错误率$E_{out}(h)$接近是由Hoeffding不等式来保证的。注意，Hoeffding不等式只保证了$E_{in}(h)$和$E_{out}(h)$接近，但是我们希望的是$E_{out}(h)$足够小。那么我们就要对模型进行选择了。我们能做的是选择（训练）一个模型，使其$E_{in}(h)$变小，但是由于模型h经过了选择，此时Hoeffding就无法保证$E_{in}(h)$与$E_{out}(h)$接近了，这就是选择性偏差的现象。

举个例子，我们举办一个万人抛硬币的比赛，一共抛5次，对于连续5次抛得正面的人，我们给予办法奖金。对于获得奖金的人，我们并不能说他们天赋异禀，也不能说他们的硬币比较特别，因为硬币就是正反相同概率的。

事实上，我们可以计算有人可以拿到奖金（连续5次正面）的人的概率，5次抛硬币一共有32个组合，其中有31种组合不是连续5次正面的，则有人能拿到奖金的概率为$(1-\frac{31}{32})^{10000}\approx 1$

对于模型的选择来说，上面拿到奖金的人好比我们经过训练得到一个$E_{in}(h)$很小的模型，但是它可能与$E_{out}(h)$差别很大，我们用BAD表示这种情况。在下面的表格中，每一列是一个数据集，我们使用的数据集是其中的某一列。每一行表示一个模型，训练算法根据数据集，在各个模型中进行选择。

Hoeffding不等式保证的是，每一模型在数据集上的误差$E_{in}(h_i)$和样本外的误差$E_{out}(h_i)$差别不大。但是总有一些数据集会导致模型$h_i$产生了样本内和样本外的误差大的现象（例如$h_i$在$D_1$和$D_{5678}$，这种情况很少）。假设我们手里的数据集是$D_{1126}$，这份数据集在所有的模型（每一行）中都不会产生BAD（即$E_{in}$和$E_{out}$差别大）的情况，那么训练算法可以自由自在的选择模型来最小化$E_{in}$而不用担心BAD的情况。反之如果我们手中的数据集是$D_1$，它会导致某些模型会产生BAD的情况，此时训练算法就有可能会踩到雷，选择有问题的模型（例如$h_3$）导致$E_{in}$和$E_{out}$差别大。因此只有当给定的样本集是$D_{1126}$这样优质的数据，算法才可以自由的选择模型，得到的模型在样本上的错误率可以表示总体的错误率。

|       | $D_1$   | $D_2$   | ...  | $D_{1126}$ | ...  | $D_{5678}$ | Hoeffding                             |
| ----- | ------- | ------- | ---- | ---------- | ---- | ---------- | ------------------------------------- |
| $h_1$ | BAD     |         |      |            |      | BAD        | $\mathbb{P}_D[\text{BAD D for } h_1]$ |
| $h_2$ |         | BAD     |      |            |      |            | $\mathbb{P}_D[\text{BAD D for } h_2]$ |
| $h_3$ | BAD     | BAD     |      |            |      | BAD        | $\mathbb{P}_D[\text{BAD D for } h_3]$ |
| ...   |         |         |      |            |      |            |                                       |
| $h_M$ | BAD     |         |      |            |      | BAD        | $\mathbb{P}_D[\text{BAD D for } h_M]$ |
| all   | **BAD** | **BAD** |      |            |      | **BAD**    | X                                     |

我们这里假设了模型集合是有限数量的，为M个。如果一个样本集，在M个模型中的任意一个是有问题的，那我们就认为这个样本集是不好的。

我们可以计算算法任意选择出的模型h，它在样本和总体上的错误率差别很大的概率的上界为
$$
\begin{aligned}
\mathbb{P}_{\mathcal{D}}[\text { BAD } \mathcal{D}]
&=\mathbb{P}_{\mathcal{D}}\left[\mathrm{BAD } \mathcal{D} \text { for } h_{1} \text { or } \mathrm{BAD } \mathcal{D} \text { for } h_{2} \text { or } \ldots \text { or } \mathrm{BAD} \mathcal{D} \text { for } h_{M}\right] \\
&\leq \mathbb{P}_{\mathcal{D}}\left[\mathrm{BAD } \mathcal{D} \text { for } h_{1}\right]+\mathbb{P}_{\mathcal{D}}\left[\mathrm{BAD} \mathcal{D} \text { for } h_{2}\right]+\ldots+\mathbb{P}_{\mathcal{D}}\left[\mathrm{BAD} \mathcal{D} \text { for } h_{M}\right]
\\
&\text{(union bound)} \\
&\leq 2 \exp \left(-2 \epsilon^{2} N\right)+2 \exp \left(-2 \epsilon^{2} N\right)+\ldots+2 \exp \left(-2 \epsilon^{2} N\right)
\\ 
&=2 M \exp \left(-2 \epsilon^{2} N\right)
\end{aligned}
$$
所以**当算法可以选择的模型数量有限**，为M时，Hoeffding的上界扩大M倍，**同时当样本集的数量N足够大**时，算法还是可以选择到一个不错的模型h，使得样本内的错误率$E_{in}$与总体的错误率$E_{out}$接近。同时，我们可以通过训练，找到一个$E_{in} \approx 0$的模型，则说明机器学习是可行的。

我们总结一下，机器学习需要解决两个问题

1. $E_{in} \approx E_{out}$
2. $E_{in}$足够小

其中，当M很小时，第1个问题我们可以通过Hoeffding来保证。第二个问题需要我们有一个很大的M，否则算法可能无法找到$E_{in}$很小的模型。因此，一个折中大小的M显得很重要。这个问题我们在下一篇讨论。

