# 主题模型

今天来聊聊主题模型，这也是一个很好的把EM算法，贝叶斯理论串起来的好素材。在主题模型领域，LDA模型被人们熟知，但是在讲LDA之前，要先介绍一下它的前身pLSA。

## 1. pLSA

pLSA的全称叫做probabilistic Latent Semantic Analysis，概率语义分析模型。pLSA是一种简单的贝叶斯网络，可以用下面这幅图来描述。

![Image text](https://raw.github.com/Casey1203/ml-ease/master/img/plsa.png)

定义$D$表示文档集合，$Z$表示主题集合，$W$表示词语集合。

定义概率$P(d_i)$表示文档$d_i$出现的概率；$P(z_k|d_i)$表示给定文档$d_i$，主题$z_k$出现的概率；$P(w_j|z_k)$表示给定主题$z_k$，词语$w_j$出现的概率。

每个主题在词语上服从多项分布，每个文档在主题上服从多项分布，即$P(z_k|d_i)$和$P(w_j|z_k)$均服从多项分布。假设一共有$V$个词，$K$个主题，则$P(z_k|d_i)$服从$K$点分布，$P(w_j|z_k)$服从$V$点分布。

给定一个语料库，里面有很多篇文档，可以观察词语和文档的对，第$i$篇文档的第$j$个词语$(d_i,w_j)$，它的联合概率分布可以表示为
$$
P(d_i,w_j)=P(w_j|d_i)P(d_i)
$$
而给定文档$d_i$，词语$w_j$的概率$P(w_j|d_i)$可以写成
$$
P(w_j|d_i)=\sum_{k=1}^KP(w_j|z_k)P(z_k|d_i)
$$
这里多说一句，$P(w_j|z_k)=P(w_j|z_k,d_i)$，可以发现给不给$d_i$不影响$w_j$，因为已经给定了$z_k$，那么$d_i$和$w_j$就是一个head-to-tail的关系，这两个互相独立了。因此有这个等式关系。

可以发现$\sum_{k=1}^K$中的两项条件概率就是上文提到的给定文档的主题分布$P(z_k|d_i)$和给定主题的词分布$P(w_j|z_k)$。

因此为了求解这两个分布的参数，首先写出似然函数以及对数似然函数
$$
L=\prod_{i=1}^N{\prod_{j=1}^M}{P(d_i,w_j)}=\prod_{i}{\prod_j}{P(d_i,w_j)^{n(d_i,w_j)}}
$$
其中$N$和$M$分别表示语料库中文档的个数和词语的个数，$n(d_i,w_j)$表示$d_i$和$w_j$的文档-词语对的个数。

对数似然函数为
$$
\begin{aligned}
l &= \sum _ { i } \sum _ { j } n \left( d _ { i } , w _ { j } \right) \log P \left( d _ { i } , w _ { j } \right) \\
&= \sum _ { i } \sum _ { j } n \left( d _ { i } , w _ { j } \right) \log \left(P \left( w _ { j } | d _ { i } \right) P \left( d _ { i } \right)\right) \\

&=\sum _ { i } \sum _ { j } n \left( d _ { i } , w _ { j } \right) \log \left( \sum _ { k = 1 } ^ { K } P \left( w _ { j } | z _ { k } \right) P \left( z _ { k } | d _ { i } \right) P \left( d _ { i } \right) \right)
\end{aligned}
$$
观测对数似然函数，发现包含隐变量$z_k$，因此采用EM算法求解该似然函数最大。

E步，求解隐变量的后验概率$P(z_k|w_j,d_i)$。假设$P(z_k|d_i)$和$P(w_j|z_k)$是已知的，那$P(z_k|w_j,d_i)$可以写成
$$
P(z_k|w_j,d_i)=\frac{P(w_j|z_k)P(z_k|d_i)=P(w_j,z_k|d_i)}{\sum_{l=1}^K{P(w_j|z_l)P(z_l|d_i)}=P(w_j|d_i)}
$$
M步