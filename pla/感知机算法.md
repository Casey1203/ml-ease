**感知机算法**

打分和阈值模型
$$
h(\mathbf{x})=\operatorname{sign}\left(\left(\sum_{i=1}^{d} w_{i} x_{i}\right)-\text { threshold }\right)
$$
对该模型进行简化
$$
\begin{aligned}
h(\mathbf{x}) &=\operatorname{sign}\left(\left(\sum_{i=1}^{d} w_{i} x_{i}\right)-\text { threshold }\right) \\
&=\operatorname{sign}(\left(\sum_{i=1}^{d} w_{i} x_{i}\right)+\underbrace{(-\text { threshold })}_{w_{0}} \cdot \underbrace{(+1)}_{x_{0}}) \\
&=\operatorname{sign}\left(\sum_{i=0}^{d} w_{i} x_{i}\right) \\
&=\operatorname{sign}\left(\mathbf{w}^{T} \mathbf{x}\right)
\end{aligned}
$$
这样就不需要显示的写出threshold了，这就是PLA的模型形式。

PLA的算法过程（Cyclic PLA）

对$t=0,1, \ldots$

当前模型的参数$\mathbf{w}_t$，找到错误的样本$$\left(\mathbf{x}_{n(t)}, y_{n(t)}\right)$$，有
$$
\operatorname{sign}\left(\mathbf{w}_{t}^{T} \mathbf{x}_{n(t)}\right) \neq y_{n(t)}
$$
利用下式来纠正这个错误
$$
\mathbf{w}_{t+1} \leftarrow \mathbf{w}_{t}+y_{n(t)} \mathbf{x}_{n(t)}
$$
对$t$进行迭代，知道没有样本分类错误。

找样本的方法可以顺序，也可以随机。

![pla](https://i.loli.net/2020/07/23/K9G6JdAlDPosnRe.gif)

PLA算法有以下的性质，对于t轮和t+1轮的模型参数，在同一个样本点上满足
$$
y_{n} \mathbf{w}_{t+1}^{T} \mathbf{x}_{n} \geq y_{n} \mathbf{w}_{t}^{T} \mathbf{x}_{n}
$$
证明
$$
\begin{aligned}
y_{n} \mathbf{w}_{t+1}^{T} \mathbf{x}_{n} &= y_n(\mathbf{w}_{t}+y_{n} \mathbf{x}_n)^T \mathbf{x}_n \\
&=y_n\mathbf{w}_t^T \mathbf{x}_n+y_n^2 \mathbf{x}_n^T\mathbf{x}_n \\ & \geq y_n\mathbf{w}_t^T \mathbf{x}_n \quad \text{因为$y_n^2 \mathbf{x}_n^T\mathbf{x}_n$恒$\geq 0$}
\end{aligned}
$$
这个性质说明了，$\mathbf{w}^T \mathbf{x}_n$表示模型给样本的打分，当分数的符号与样本真实的符号$y_n$方向不同时，$y_{n} \mathbf{w}^{T} \mathbf{x}_{n}$在迭代的时候，下一轮的$\mathbf{w}$会提升$y_{n} \mathbf{w}^{T} \mathbf{x}_{n}$，更加靠近正值，即模型的打分符号更靠近$y_n$的符号。



为了证明PLA每次迭代获得的$\mathbf{w}_t$确实是比原来的好，我们来定义$\mathbf{w}_f$，为真实但是无法观测到的模型参数，所有的样本都是来自于$\mathbf{w}_f$所产生的。那么在用PLA算法得到的$\mathbf{w}_t$，如果越来越靠近$\mathbf{w}_f$，则说明该算法真的有在学习。该证明分成两部分。

1. 为了描述“靠近”，我们用向量的内积表示，即$\mathbf{w}_t^T \mathbf{w}_f$，我们想要看下面的式子是否成立

$$
\mathbf{w}^T_{t+1}\mathbf{w}_f \geq \mathbf{w}^T_{t}\mathbf{w}_f
$$

我们将$\mathbf{w}_{t+1} \leftarrow \mathbf{w}_{t}+y_{n} \mathbf{x}_{n}$带入得到
$$
\begin{aligned}
\mathbf{w}_{t+1}^T \mathbf{w}_f=(\mathbf{w}_{t}+y_{n} \mathbf{x}_{n})^T\mathbf{w}_f &=\mathbf{w}^T_t\mathbf{w}_f+y_n\mathbf{x}_n^T \mathbf{w}_f \\
&\geq \mathbf{w}^T_t\mathbf{w}_f + \min_n y_n\mathbf{x}_n^T \mathbf{w}_f \\
& \geq \mathbf{w}_0^T \mathbf{w}_f + (t+1)\min_n y_n\mathbf{x}_n^T \mathbf{w}_f
\quad \text{迭代t+1次} \\
& =(t+1)\min_n y_n\mathbf{x}_n^T \mathbf{w}_f \quad \text{$\mathbf{w}_0$初始化为0}
\end{aligned}
$$
由于$\mathbf{w}_f$是真实的模型参数，因此对所有样本都有正确的分类（记住PLA可以找到$\mathbf{w}_f$的前提条件是线性可分的数据集），即$ \min_n y_n\mathbf{x}_n^T \mathbf{w}_f\geq 0$。所以上面的式子满足
$$
(\mathbf{w}_{t}+y_{n} \mathbf{x}_{n})^T\mathbf{w}_f =\mathbf{w}_{t+1}^T\mathbf{w}_f \geq \mathbf{w}^T_t\mathbf{w}_f
$$
说明每次迭代，可以让当前获得的模型估计的参数向量，与真实的参数向量的内积更大。

2. 但是考虑到向量具有长度，因此还需要将向量对其模做归一化。我们考察每次更新，对参数向量的模的影响

$$
\begin{aligned}
\left\|\mathbf{w}_{t+1}\right\|^{2} &=\left\|\mathbf{w}_{t}+y_{n(t)} \mathbf{x}_{n(t)}\right\|^{2} \\
&=\left\|\mathbf{w}_{t}\right\|^{2}+2 y_{n(t)} \mathbf{w}_{t}^{T} \mathbf{x}_{n(t)}+\left\|y_{n(t)} \mathbf{x}_{n(t)}\right\|^{2} \\
& \leq\left\|\mathbf{w}_{t}\right\|^{2}+0+\left\|y_{n(t)} \mathbf{x}_{n(t)}\right\|^{2} \quad \text{因为$t$轮的模型$\mathbf{w}_t$将样本$\mathbf{x}_{n(t)}$分错了,所以$y_{n(t)}\mathbf{w}_t^T\mathbf{x}_{n(t)}<0$} \\
& \leq\left\|\mathbf{w}_{t}\right\|^{2}+\max _{n}\left\|y_{n} \mathbf{x}_{n}\right\|^{2} \quad \text{模中的$y_n$可以忽略，因为它取值为$\pm1$},平方后都等于1\\
& \leq\left\|\mathbf{w}_{0}\right\|^{2}+(t+1)\max _{n}\left\| \mathbf{x}_{n}\right\|^{2} \quad \text{迭代$t+1$次} \\
&= (t+1)\max _{n}\left\| \mathbf{x}_{n}\right\|^{2} \quad \text{$\mathbf{w}_0$初始化为0}
\end{aligned}
$$

第3行用到了PLA的性质，即PLA是遇到了错误的样本，才进行更新，即样本和模型参数满足$y_{n(t)} \mathbf{w}_{t}^{T} \mathbf{x}_{n(t)} \leq 0$

第4行则用最大的$\|\mathbf{x}_n\|^2$代替，里面的$y_n$可以省略，因为$y_n$的取值为正负1，因此平方后没有影响。同时也说明，每一轮迭代，模型参数的模的增加量，最大不超过$\max _{n}\left\|\mathbf{x}_{n}\right\|^{2}$，说明模是有限制的在增长的。

结合以上两步的结论，我们可以得到以下结论，在经历了$T$轮的更新后，模型参数归一化后的向量，和真实归一化的参数的内积，随着轮次的根号级别在增长，即
$$
\frac{\mathbf{w}_{f}^{T}}{\left\|\mathbf{w}_{f}\right\|} \frac{\mathbf{w}_{T}}{\left\|\mathbf{w}_{T}\right\|} \geq \sqrt{T} \cdot \text { constant }
$$
证明如下
$$
\begin{aligned}
\frac{\mathbf{w}_{f}^{T}}{\left\|\mathbf{w}_{f}\right\|} \frac{\mathbf{w}_{T}}{\left\|\mathbf{w}_{T}\right\|} &= \frac{\mathbf{w}_{f}^{T}\mathbf{w}_{T}}{\left\|\mathbf{w}_{f}\right\|\left\|\mathbf{w}_{T}\right\|} \\
& \geq \frac{T\min_n y_n\mathbf{x}_n^T \mathbf{w}_f}{\left\|\mathbf{w}_{f}\right\| \sqrt{T}\max _{n}\left\| \mathbf{x}_{n}\right\|} \\
&= \frac{\sqrt{T}\min_n y_n \mathbf{w}_f^T \mathbf{x}_n}{\left\|\mathbf{w}_{f}\right\| \max _{n}\left\| \mathbf{x}_{n}\right\|} = \sqrt{T} \cdot \text{constant}
\end{aligned}
$$
由于两个归一化的向量的内积最多不超过1（两个向量夹脚的余弦值），因此可以将上式调整，得到
$$
T \leq \frac{\max _{n}\left\| \mathbf{x}_{n}\right\|^2\left\|\mathbf{w}_{f}\right\|^2}{\min_n^2 y_n \mathbf{w}_f^T \mathbf{x}_n}=\frac{R^2}{\rho^2}
$$
说明迭代次数$T$是有上界的。



上面证明了PLA会收敛，但是如果**线性不可分**的数据集（例如原始的数据中混入了noise），则需要改造PLA算法。

**Pocket Algorithm**

初始化一个pocket weight $\hat{\mathbf{w}}$

对$t=0,1, \ldots$

在数据集中随机找一个样本$(\mathbf{x}_{n(t)},y_{n(t)})$，无法被模型$\mathbf{w}_t$分对。

利用该样本修正模型参数
$$
\mathbf{w}_{t+1} \leftarrow \mathbf{w}_{t}+y_{n(t)} \mathbf{x}_{n(t)}
$$
如果$\mathbf{w}_{t+1}$比$\hat{\mathbf{w}}$在数据集中犯错误数更少，则用$\mathbf{w}_{t+1}$进行替代。

通过足够的迭代后，返回$\hat{\mathbf{w}}$。

这个算法的缺点是，速度比PLA要慢，因为它每一轮都要在**全部样本**上看是否有犯错误。

